# Hackup du 26 mars 2025 / Samuel Loup

## ğŸ¯ Objectifs du jour

DÃ©mystifier les LLMs : comprendre sans se perdre dans la complexitÃ©.

Se concentrer sur les bases avant dâ€™utiliser des frameworks.

AcquÃ©rir les concepts et le vocabulaire essentiels.

Montrer comment les agents peuvent simplifier lâ€™orchestration des LLMs (use-case dans le logiciel Yeti).

# La thÃ©orie

## ğŸš€ Un LLM, ce nâ€™est pas de la magie, câ€™est juste des maths

ğŸ”¹ Un LLM est une grosse calculatrice statistique qui prÃ©dit le mot suivant en fonction dâ€™un contexte donnÃ©.

## ğŸ§  Les LLMs ne comprennent rien, ils imitent des patterns

ğŸ”¹ Un LLM ne comprend pas ce quâ€™il dit, il imite statistiquement ce quâ€™il a appris.

## L'oiseau bleu

```
Quand je dis l'oiseau, vous dite bleu...
```

## ğŸ” Local vs Cloud â€“ OÃ¹ tourne un LLM ?

ğŸ”¹ Un LLM peut Ãªtre hÃ©bergÃ© sur un serveur distant (OpenAI, Claude, Gemini) ou tournÃ© en local sur un PC.

ğŸ”‘ LLM â‰  API, il y a plusieurs faÃ§ons de les faire tourner.

# ğŸ¤— Hugginface

Hugging Face est la plateforme centrale pour les LLMs, agissant comme un GitHub dÃ©diÃ© aux modÃ¨les de langage et autres technologies dâ€™intelligence artificielle. Il permet de partager, tester, fine-tuner, et dÃ©ployer des modÃ¨les tout en offrant un environnement de collaboration et dâ€™innovation continue pour les chercheurs, les entreprises et la communautÃ© open-source.

C'est un lieu incontournable pour travailler avec des modÃ¨les prÃ©-entrainÃ©s et rester Ã  jour avec les derniÃ¨res avancÃ©es dans le domaine des LLMs.

# ğŸ§¬ Nomenclature des ModÃ¨les LLM

Nom de base : DÃ©signe l'architecture ou le type de modÃ¨le (GPT, Llama, Mistral).

Taille : Indique le nombre de paramÃ¨tres, par exemple, 7B pour 7 milliards de paramÃ¨tres.

Optimisation : Des termes comme quantized, flash ou turbo dÃ©signent des versions optimisÃ©es.

SpÃ©cificitÃ© : Des ajouts comme **embed**, **instruct** ou **multilingual** indiquent des spÃ©cifications

## ğŸ† C'est pas la taille qui compte

### small

Exemple : **mistral-small** (22 millions de paramÃ¨tres)

UtilisÃ© pour des tÃ¢ches lÃ©gÃ¨res avec des ressources limitÃ©es, mais avec des performances acceptables sur des tÃ¢ches simples.

### base

Exemple : **llama-7B** (7 milliards de paramÃ¨tres)

ModÃ¨les adaptÃ©s aux serveurs ou aux dÃ©ploiements avec une puissance de calcul plus consÃ©quente. Bonne performance sur une large gamme de tÃ¢ches, tout en restant relativement lÃ©ger.

### large

Exemple : **gpt-3** (175 milliards de paramÃ¨tres), llama-30B (30 milliards de paramÃ¨tres).

Ces modÃ¨les nÃ©cessitent une grande quantitÃ© de mÃ©moire GPU pour Ãªtre exÃ©cutÃ©s efficacement. Ils offrent des performances exceptionnelles sur des tÃ¢ches complexes et offrent un excellent traitement contextuel.

### mega-large

Exemple : **gpt-4** (100+ milliards de paramÃ¨tres).

Ces modÃ¨les sont utilisÃ©s pour des applications de pointe mais nÃ©cessitent une infrastructure matÃ©rielle robuste (clusters de serveurs, GPU haut de gamme, etc.).

## Quantization

La quantization est un processus qui rÃ©duit la taille des modÃ¨les en rÃ©duisant la prÃ©cision des poids et des activations (par exemple, passer de 32 bits Ã  16 bits ou 8 bits), permettant des performances plus rapides tout en rÃ©duisant l'empreinte mÃ©moire.

### Impact de la Quantization

Un modÃ¨le quantized consomme moins de mÃ©moire (par exemple, 8 GB au lieu de 16 GB) et **s'exÃ©cute plus vite**, surtout sur des GPU ou des processeurs spÃ©cialisÃ©s dans les calculs Ã  faible prÃ©cision (comme les Tensor Cores des GPUs NVIDIA).

La perte de prÃ©cision est souvent nÃ©gligeable dans des tÃ¢ches pratiques, mais peut affecter des tÃ¢ches plus spÃ©cifiques oÃ¹ la prÃ©cision est cruciale.

## SpÃ©cificitÃ©s

Les modÃ¨les **instruct** sont conÃ§us pour suivre des instructions spÃ©cifiques, et sont entraÃ®nÃ©s avec des exemples oÃ¹ l'objectif est de gÃ©nÃ©rer des rÃ©ponses prÃ©cises et utiles en fonction de ce qui est demandÃ©. Les modÃ¨les gÃ©nÃ©ratifs classiques, quant Ã  eux, se concentrent plus sur la fluiditÃ© de la gÃ©nÃ©ration de texte sans forcÃ©ment respecter des consignes prÃ©cises.

# ProblÃ©matique de la taille du contexte et du nombre de tokens

Un token est une unitÃ© de texte (mot, morceau de mot ou caractÃ¨re).

La taille du contexte dâ€™un modÃ¨le LLM correspond au nombre maximal de tokens quâ€™il peut traiter en une seule requÃªte.

Limite de mÃ©moire : Plus le contexte est long, plus il faut de mÃ©moire GPU/CPU pour l'infÃ©rence.

Perte dâ€™information : Si le contexte dÃ©passe la limite, lâ€™excÃ¨s de tokens est tronquÃ© (en gÃ©nÃ©ral Ã  gauche).

DÃ©gradation des performances : Certains modÃ¨les gÃ¨rent mal les contextes longs et oublient des infos en dÃ©but de texte.

# Le format GGUF

Le format GGUF est souvent utilisÃ© pour stockage et Ã©change de modÃ¨les de machine learning, en particulier dans des scÃ©narios oÃ¹ plusieurs composants doivent interagir, comme les rÃ©seaux de neurones et les moteurs d'infÃ©rence.

Il permet d'optimiser la performances computationnelles tout en facilitant le stockage, l'Ã©change, et le dÃ©ploiement de modÃ¨les. GrÃ¢ce Ã  son approche basÃ©e sur des graphes et Ã  sa compatibilitÃ© avec divers environnements matÃ©riels, GGUF est une solution clÃ© pour la gestion des modÃ¨les modernes dans le domaine de l'IA.

[https://huggingface.co/mradermacher](https://huggingface.co/mradermacher)

# Architectures

**Transformer** : Architecture de base pour tous les modÃ¨les modernes (GPT, BERT, T5, etc.).

**BERT** : Bidirectionnel, efficace pour la comprÃ©hension du langage et les tÃ¢ches de classification.

**GPT** : Unidirectionnel, spÃ©cialisÃ© dans la gÃ©nÃ©ration de texte.

**T5** : Architecture text-to-text, trÃ¨s flexible pour les tÃ¢ches NLP.

**XLNet** : MÃ©lange de BERT et GPT, avec une meilleure gestion des dÃ©pendances.

**BART** : CombinÃ© de BERT et GPT, adaptÃ© pour des tÃ¢ches de gÃ©nÃ©ration et de rÃ©sumÃ©.

**Turing-NLG** : GÃ©nÃ©ration de texte de haute qualitÃ© et cohÃ©rent.

**Reformer** : Optimisation des Transformers pour les sÃ©quences longues.

**Albert** : Version lÃ©gÃ¨re de BERT, plus rapide et moins gourmande en ressources.

**Llama** : OptimisÃ© pour des modÃ¨les plus petits avec des performances de haute qualitÃ©.

## Llama

Description : Llama est une architecture Transformer dÃ©veloppÃ©e par Meta (anciennement Facebook). Elle est conÃ§ue pour offrir des performances compÃ©titives tout en Ã©tant plus lÃ©gÃ¨re que les plus grands modÃ¨les de la sÃ©rie GPT-3 et GPT-4. Llama est une architecture open-source permettant une flexibilitÃ© d'usage.

Exemples de modÃ¨les : Llama-7B, Llama-13B, Llama-30B.

# InfÃ©rer un modÃ¨le

## Monter une infrastructure locale avec Ollama

[https://ollama.com/](https://ollama.com/)

```bash
ollama pull <model de votre choix>
```

```bash
ollama run mistral-nemo
```

### Ollama API

[api.md](https://github.com/ollama/ollama/blob/main/docs/api.md)

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "mistral-nemo",
  "prompt":"Salut Mistral !"
}'
```

```bash
curl http://localhost:11434/api/chat -d '{
  "model": "mistral-nemo",
  "messages": [
    { "role": "user", "content": "Salut Mistral !" }
  ]
}'
```

```bash
curl http://localhost:11434/api/embed -d '{
  "model": "granite-embedding:278m",
  "input": "Salut Mistral !"
}'
```

## Utiliser le Cloud

### OpenAI API

[https://platform.openai.com/docs/api-reference/introduction](https://platform.openai.com/docs/api-reference/introduction)

### Openrouter.ai

[https://openrouter.ai/docs/quickstart](https://openrouter.ai/docs/quickstart)

```bash
curl https://openrouter.ai/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENROUTER_API_KEY" \
  -d '{
  "model": "openai/gpt-4o",
  "messages": [
    {
      "role": "user",
      "content": "What is the meaning of life?"
    }
  ]
}'

```

## Utiliser une lib

## Avec llama.cpp

[https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)

# Avec node-llama

[https://node-llama-cpp.withcat.ai/](https://node-llama-cpp.withcat.ai/)

```bash
npx --no node-llama-cpp inspect gpu
```

# IntÃ©gration de l'IA dans Yeti avec Xcraft

## Xcraft - Le framework maison

[ğŸ“˜ documentation](../../../doc/autogen/xcraft-core-goblin/readme.md)

## Agents - Les agents maison

[ğŸ§  un agent](../../../lib/goblin-agents/lib/llm/yetiAgent.js)

[ğŸ“˜ documentation](../../../doc/autogen/goblin-agents/llm.md)

## Cryo - Le data-layer maison

[ğŸ“˜ documentation](../../../doc/autogen/xcraft-core-goblin/cryo.md)

### Embeddings

```sql
CREATE VIRTUAL TABLE IF NOT EXISTS embeddings768 USING vec0(
    documentId TEXT,
    chunkId TEXT,
    chunk TEXT,
    embedding FLOAT[768] distance_metric=cosine
)


SELECT documentId, chunkId, chunk, distance
      FROM embeddings768
      WHERE embedding match $vectors
      ORDER BY distance LIMIT $limit
```

## Indexeur de contenu

```js
class ChunkShape {
  chunk = string;
  embedding = array;
}

class MetaShape {
  index = option(string); //FullTextSearch
  status = enumeration('published', 'trashed', 'archived');
  vectors = option(record(string, ChunkShape)); //SQLITE-VEC
}

class IndexedContentShape {
  id = id('indexedContent');
  kind = string;
  title = string;
  description = string;
  contextId = string;
  sourceId = string;
  relatedIds = array(string);
  weight = number;
  meta = MetaShape;
}
```

[ğŸ§ª indexeur de contenu](../../../lib/goblin-yennefer/lib/indexer.js)

## Articles wordpress

[ğŸ“œ ragCresus](../../../lib/goblin-yennefer/lib/prompts/ragCresus.md)

[ğŸ§ª processus RAG dans wordpress](../../../lib/goblin-epsilon/lib/wordpress/wordpress.js)

[ğŸ“˜ documentation de la partie Wordpress](../../../doc/autogen/goblin-epsilon/wordpress.md)

## Agents de bases

[ğŸ§  dÃ©finition des agents de base](../../../lib/goblin-yennefer/lib/builtInAgents.js)

[ğŸ“˜ documentation de la partie LLM](../../../doc/autogen/goblin-yennefer/llm.md)

## GÃ©nÃ©rer de la doc

[ğŸ“œ codeAnalyser](../../../lib/goblin-yennefer/lib/prompts/codeAnalyser.md)

[ğŸ§ª gÃ©nÃ©rateur de doc](../../../lib/goblin-yennefer/lib/codeMiner.js)

## GÃ©nÃ©rer un dataset

[ğŸ“œ qaMinerDatasetBuilder](../../../lib/goblin-yennefer/lib/prompts/qaMinerDatasetBuilder.md)

[ğŸ§ª gÃ©nÃ©rateur de Question&Answer ](../../../lib/goblin-yennefer/lib/qaMiner.js)

## GÃ©nÃ©ration de tÃ¢ches et analyse de cas

[ğŸ“œ tasksProposalAgent](../../../lib/goblin-yennefer/lib/prompts/tasksProposalAgent.md)

[ğŸ“œ agentOrchestrator](../../../lib/goblin-yennefer/lib/prompts/agentOrchestrator.md)

[ğŸ“œ promptGen](../../../lib/goblin-yennefer/lib/prompts/promptGen.md)

[ğŸ§ª analyse de cas](../../../lib/goblin-yeti/lib/widgets/case-workitem/service.js)

[ğŸ“˜ documentation du case-workitem](../../../doc/autogen/goblin-yeti/case-workitem.md)

## Dans le journal

[ğŸ§ª commander des agents](../../../lib/goblin-yeti/lib/widgets/journal/service.js)

[ğŸ“˜ documentation du journal](../../../doc/autogen/goblin-yeti/journal.md)

# Bonus : extraction et analyse de PDF

```js
//TODO
```
