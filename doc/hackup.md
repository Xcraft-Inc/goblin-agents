# Hackup Epsitec du 26 mars 2025

## üéØ Objectifs du jour

1Ô∏è‚É£ D√©mystifier les LLMs : comprendre sans se perdre dans la complexit√©.

2Ô∏è‚É£ Infrastructure : moteurs d‚Äôinf√©rence et les diff√©rents modes d‚Äôex√©cution.

3Ô∏è‚É£ Se concentrer sur les bases avant d‚Äôutiliser des frameworks.

4Ô∏è‚É£ Montrer comment les agents peuvent simplifier l‚Äôorchestration des LLMs.

5Ô∏è‚É£ Acqu√©rir les concepts et le vocabulaire essentiels.

# La th√©orie

## üöÄ Un LLM, ce n‚Äôest pas de la magie, c‚Äôest juste des maths

üîπ Un LLM est une grosse calculatrice statistique qui pr√©dit le mot suivant en fonction d‚Äôun contexte donn√©.

## üß† Les LLMs ne comprennent rien, ils imitent des patterns

üîπ Un LLM ne comprend pas ce qu‚Äôil dit, il statistiquement imite ce qu‚Äôil a appris.

## L'oiseau bleu

```
Quand je dis l'oiseau vous dite bleu...
```

## üîç Local vs Cloud ‚Äì O√π tourne un LLM ?

üîπ Un LLM peut √™tre h√©berg√© sur un serveur distant (OpenAI, Claude, Gemini) ou tourn√© en local sur un PC.

üîπ Vocabulaire : Inf√©rence, quantization, acc√©l√©ration GPU/CPU.

üîë LLM ‚â† API, il y a plusieurs fa√ßons de les faire tourner.

# ü§ó Hugginface

Hugging Face est la plateforme centrale pour les LLMs, agissant comme un GitHub d√©di√© aux mod√®les de langage et autres technologies d‚Äôintelligence artificielle. Il permet de partager, tester, fine-tuner, et d√©ployer des mod√®les tout en offrant un environnement de collaboration et d‚Äôinnovation continue pour les chercheurs, les entreprises et la communaut√© open-source.

C'est un lieu incontournable pour travailler avec des mod√®les pr√©-entrain√©s et rester √† jour avec les derni√®res avanc√©es dans le domaine des LLMs.

# üß¨ Nomenclature des Mod√®les LLM

Nom de base : D√©signe l'architecture ou le type de mod√®le (GPT, Llama, Mistral).

Taille : Indique le nombre de param√®tres, par exemple, 7B pour 7 milliards de param√®tres.

Optimisation : Des termes comme quantized, flash ou turbo d√©signent des versions optimis√©es.

Sp√©cificit√© : Des ajouts comme **embed**, **instruct** ou **multilingual** indiquent des sp√©cifications

## üçÜ C'est pas la taille qui compte

### small

Exemple : **mistral-small** (22 millions de param√®tres)

Utilis√© pour des t√¢ches l√©g√®res avec des ressources limit√©es, mais avec des performances acceptables sur des t√¢ches simples.

### base

Exemple : **llama-7B** (7 milliards de param√®tres)

Mod√®les adapt√©s aux serveurs ou aux d√©ploiements avec une puissance de calcul plus cons√©quente. Bonne performance sur une large gamme de t√¢ches, tout en restant relativement l√©ger.

### large

Exemple : **gpt-3** (175 milliards de param√®tres), llama-30B (30 milliards de param√®tres).

Ces mod√®les n√©cessitent une grande quantit√© de m√©moire GPU pour √™tre ex√©cut√©s efficacement. Ils offrent des performances exceptionnelles sur des t√¢ches complexes et offrent un excellent traitement contextuel.

### mega-large

Exemple : **gpt-4** (100+ milliards de param√®tres).

Ces mod√®les sont utilis√©s pour des applications de pointe mais n√©cessitent une infrastructure mat√©rielle robuste (clusters de serveurs, GPU haut de gamme, etc.).

## Quantization

La quantization est un processus qui r√©duit la taille des mod√®les en r√©duisant la pr√©cision des poids et des activations (par exemple, passer de 32 bits √† 16 bits ou 8 bits), permettant des performances plus rapides tout en r√©duisant l'empreinte m√©moire.

### Impact de la Quantization

Un mod√®le quantized consomme moins de m√©moire (par exemple, 8 GB au lieu de 16 GB) et **s'ex√©cute plus vite**, surtout sur des GPU ou des processeurs sp√©cialis√©s dans les calculs √† faible pr√©cision (comme les Tensor Cores des GPUs NVIDIA).

La perte de pr√©cision est souvent n√©gligeable dans des t√¢ches pratiques, mais peut affecter des t√¢ches plus sp√©cifiques o√π la pr√©cision est cruciale.

## Sp√©cificit√©s

Les mod√®les **instruct** sont con√ßus pour suivre des instructions sp√©cifiques, et sont entra√Æn√©s avec des exemples o√π l'objectif est de g√©n√©rer des r√©ponses pr√©cises et utiles en fonction de ce qui est demand√©. Les mod√®les g√©n√©ratifs classiques, quant √† eux, se concentrent plus sur la fluidit√© de la g√©n√©ration de texte sans forc√©ment respecter des consignes pr√©cises.

# Le format GGUF

Le format GGUF est souvent utilis√© pour stockage et √©change de mod√®les de machine learning, en particulier dans des sc√©narios o√π plusieurs composants doivent interagir, comme les r√©seaux de neurones et les moteurs d'inf√©rence.

Il permet d'optimiser la performances computationnelles tout en facilitant le stockage, l'√©change, et le d√©ploiement de mod√®les. Gr√¢ce √† son approche bas√©e sur des graphes et √† sa compatibilit√© avec divers environnements mat√©riels, GGUF est une solution cl√© pour la gestion des mod√®les modernes dans le domaine de l'IA.

[https://huggingface.co/mradermacher](https://huggingface.co/mradermacher)

# Architectures

Transformer : Architecture de base pour tous les mod√®les modernes (GPT, BERT, T5, etc.).

BERT : Bidirectionnel, efficace pour la compr√©hension du langage et les t√¢ches de classification.

GPT : Unidirectionnel, sp√©cialis√© dans la g√©n√©ration de texte.

T5 : Architecture text-to-text, tr√®s flexible pour les t√¢ches NLP.

XLNet : M√©lange de BERT et GPT, avec une meilleure gestion des d√©pendances.

BART : Combin√© de BERT et GPT, adapt√© pour des t√¢ches de g√©n√©ration et de r√©sum√©.

Turing-NLG : G√©n√©ration de texte de haute qualit√© et coh√©rent.

Reformer : Optimisation des Transformers pour les s√©quences longues.

Albert : Version l√©g√®re de BERT, plus rapide et moins gourmande en ressources.

Llama : Optimis√© pour des mod√®les plus petits avec des performances de haute qualit√©.

## Llama

Description : Llama est une architecture Transformer d√©velopp√©e par Meta (anciennement Facebook). Elle est con√ßue pour offrir des performances comp√©titives tout en √©tant plus l√©g√®re que les plus grands mod√®les de la s√©rie GPT-3 et GPT-4. Llama est une architecture open-source permettant une flexibilit√© d'usage.

Exemples de mod√®les : Llama-7B, Llama-13B, Llama-30B.

# La pratique

## Monter une infrastructure locale avec Ollama

[https://ollama.com/](https://ollama.com/)

```bash
ollama pull <model de votre choix>
```

```bash
ollama run mistral-nemo
```

## Ollama API

[api.md](https://github.com/ollama/ollama/blob/main/docs/api.md)

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "mistral-nemo",
  "prompt":"Salut Mistral !"
}'
```

```bash
curl http://localhost:11434/api/chat -d '{
  "model": "mistral-nemo",
  "messages": [
    { "role": "user", "content": "Salut Mistral !" }
  ]
}'
```

```bash
curl http://localhost:11434/api/embed -d '{
  "model": "all-minilm",
  "input": "Salut Mistral !"
}'
```

## Utiliser une infrastructure cloud

## OpenAI API

[https://platform.openai.com/docs/api-reference/introduction](https://platform.openai.com/docs/api-reference/introduction)

## Openrouter.ai

[https://openrouter.ai/docs/quickstart](https://openrouter.ai/docs/quickstart)

```bash
curl https://openrouter.ai/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENROUTER_API_KEY" \
  -d '{
  "model": "openai/gpt-4o",
  "messages": [
    {
      "role": "user",
      "content": "What is the meaning of life?"
    }
  ]
}'

```

# Avec Xcraft et les Elfes
