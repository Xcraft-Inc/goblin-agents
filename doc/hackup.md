# Hackup du 26 mars 2025 / Samuel Loup

## üéØ Objectifs du jour

D√©mystifier les LLMs : comprendre sans se perdre dans la complexit√©.

Se concentrer sur les bases avant d‚Äôutiliser des frameworks.

Acqu√©rir les concepts et le vocabulaire essentiels.

Montrer comment les agents peuvent simplifier l‚Äôorchestration des LLMs (use-case dans le logiciel Yeti).

# La th√©orie

## üöÄ Un LLM, ce n‚Äôest pas de la magie, c‚Äôest juste des maths

üîπ Un LLM est une grosse calculatrice statistique qui pr√©dit le mot suivant en fonction d‚Äôun contexte donn√©.

## üß† Les LLMs ne comprennent rien, ils imitent des patterns

üîπ Un LLM ne comprend pas ce qu‚Äôil dit, il imite statistiquement ce qu‚Äôil a appris.

## L'oiseau bleu

```
Quand je dis l'oiseau, vous dite bleu...
```

## üîç Local vs Cloud ‚Äì O√π tourne un LLM ?

üîπ Un LLM peut √™tre h√©berg√© sur un serveur distant (OpenAI, Claude, Gemini) ou tourn√© en local sur un PC.

üîë LLM ‚â† API, il y a plusieurs fa√ßons de les faire tourner.

# ü§ó Hugginface

Hugging Face est la plateforme centrale pour les LLMs, agissant comme un GitHub d√©di√© aux mod√®les de langage et autres technologies d‚Äôintelligence artificielle. Il permet de partager, tester, fine-tuner, et d√©ployer des mod√®les tout en offrant un environnement de collaboration et d‚Äôinnovation continue pour les chercheurs, les entreprises et la communaut√© open-source.

C'est un lieu incontournable pour travailler avec des mod√®les pr√©-entrain√©s et rester √† jour avec les derni√®res avanc√©es dans le domaine des LLMs.

# üß¨ Nomenclature des Mod√®les LLM

Nom de base : D√©signe l'architecture ou le type de mod√®le (GPT, Llama, Mistral).

Taille : Indique le nombre de param√®tres, par exemple, 7B pour 7 milliards de param√®tres.

Optimisation : Des termes comme quantized, flash ou turbo d√©signent des versions optimis√©es.

Sp√©cificit√© : Des ajouts comme **embed**, **instruct** ou **multilingual** indiquent des sp√©cifications

## üçÜ C'est pas la taille qui compte

### small

Exemple : **mistral-small** (22 millions de param√®tres)

Utilis√© pour des t√¢ches l√©g√®res avec des ressources limit√©es, mais avec des performances acceptables sur des t√¢ches simples.

### base

Exemple : **llama-7B** (7 milliards de param√®tres)

Mod√®les adapt√©s aux serveurs ou aux d√©ploiements avec une puissance de calcul plus cons√©quente. Bonne performance sur une large gamme de t√¢ches, tout en restant relativement l√©ger.

### large

Exemple : **gpt-3** (175 milliards de param√®tres), llama-30B (30 milliards de param√®tres).

Ces mod√®les n√©cessitent une grande quantit√© de m√©moire GPU pour √™tre ex√©cut√©s efficacement. Ils offrent des performances exceptionnelles sur des t√¢ches complexes et offrent un excellent traitement contextuel.

### mega-large

Exemple : **gpt-4** (100+ milliards de param√®tres).

Ces mod√®les sont utilis√©s pour des applications de pointe mais n√©cessitent une infrastructure mat√©rielle robuste (clusters de serveurs, GPU haut de gamme, etc.).

## Quantization

La quantization est un processus qui r√©duit la taille des mod√®les en r√©duisant la pr√©cision des poids et des activations (par exemple, passer de 32 bits √† 16 bits ou 8 bits), permettant des performances plus rapides tout en r√©duisant l'empreinte m√©moire.

### Impact de la Quantization

Un mod√®le quantized consomme moins de m√©moire (par exemple, 8 GB au lieu de 16 GB) et **s'ex√©cute plus vite**, surtout sur des GPU ou des processeurs sp√©cialis√©s dans les calculs √† faible pr√©cision (comme les Tensor Cores des GPUs NVIDIA).

La perte de pr√©cision est souvent n√©gligeable dans des t√¢ches pratiques, mais peut affecter des t√¢ches plus sp√©cifiques o√π la pr√©cision est cruciale.

## Sp√©cificit√©s

Les mod√®les **instruct** sont con√ßus pour suivre des instructions sp√©cifiques, et sont entra√Æn√©s avec des exemples o√π l'objectif est de g√©n√©rer des r√©ponses pr√©cises et utiles en fonction de ce qui est demand√©. Les mod√®les g√©n√©ratifs classiques, quant √† eux, se concentrent plus sur la fluidit√© de la g√©n√©ration de texte sans forc√©ment respecter des consignes pr√©cises.

# Probl√©matique de la taille du contexte et du nombre de tokens

Un token est une unit√© de texte (mot, morceau de mot ou caract√®re).

La taille du contexte d‚Äôun mod√®le LLM correspond au nombre maximal de tokens qu‚Äôil peut traiter en une seule requ√™te.

Limite de m√©moire : Plus le contexte est long, plus il faut de m√©moire GPU/CPU pour l'inf√©rence.

Perte d‚Äôinformation : Si le contexte d√©passe la limite, l‚Äôexc√®s de tokens est tronqu√© (en g√©n√©ral √† gauche).

D√©gradation des performances : Certains mod√®les g√®rent mal les contextes longs et oublient des infos en d√©but de texte.

# Le format GGUF

Le format GGUF est souvent utilis√© pour stockage et √©change de mod√®les de machine learning, en particulier dans des sc√©narios o√π plusieurs composants doivent interagir, comme les r√©seaux de neurones et les moteurs d'inf√©rence.

Il permet d'optimiser la performances computationnelles tout en facilitant le stockage, l'√©change, et le d√©ploiement de mod√®les. Gr√¢ce √† son approche bas√©e sur des graphes et √† sa compatibilit√© avec divers environnements mat√©riels, GGUF est une solution cl√© pour la gestion des mod√®les modernes dans le domaine de l'IA.

[https://huggingface.co/mradermacher](https://huggingface.co/mradermacher)

# Architectures

**Transformer** : Architecture de base pour tous les mod√®les modernes (GPT, BERT, T5, etc.).

**BERT** : Bidirectionnel, efficace pour la compr√©hension du langage et les t√¢ches de classification.

**GPT** : Unidirectionnel, sp√©cialis√© dans la g√©n√©ration de texte.

**T5** : Architecture text-to-text, tr√®s flexible pour les t√¢ches NLP.

**XLNet** : M√©lange de BERT et GPT, avec une meilleure gestion des d√©pendances.

**BART** : Combin√© de BERT et GPT, adapt√© pour des t√¢ches de g√©n√©ration et de r√©sum√©.

**Turing-NLG** : G√©n√©ration de texte de haute qualit√© et coh√©rent.

**Reformer** : Optimisation des Transformers pour les s√©quences longues.

**Albert** : Version l√©g√®re de BERT, plus rapide et moins gourmande en ressources.

**Llama** : Optimis√© pour des mod√®les plus petits avec des performances de haute qualit√©.

## Llama

Description : Llama est une architecture Transformer d√©velopp√©e par Meta (anciennement Facebook). Elle est con√ßue pour offrir des performances comp√©titives tout en √©tant plus l√©g√®re que les plus grands mod√®les de la s√©rie GPT-3 et GPT-4. Llama est une architecture open-source permettant une flexibilit√© d'usage.

Exemples de mod√®les : Llama-7B, Llama-13B, Llama-30B.

# Inf√©rer un mod√®le

## Monter une infrastructure locale avec Ollama

[https://ollama.com/](https://ollama.com/)

```bash
ollama pull <model de votre choix>
```

```bash
ollama run mistral-nemo
```

### Ollama API

[api.md](https://github.com/ollama/ollama/blob/main/docs/api.md)

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "mistral-nemo",
  "prompt":"Salut Mistral !"
}'
```

```bash
curl http://localhost:11434/api/chat -d '{
  "model": "mistral-nemo",
  "messages": [
    { "role": "user", "content": "Salut Mistral !" }
  ]
}'
```

```bash
curl http://localhost:11434/api/embed -d '{
  "model": "granite-embedding:278m",
  "input": "Salut Mistral !"
}'
```

## Utiliser le Cloud

### OpenAI API

[https://platform.openai.com/docs/api-reference/introduction](https://platform.openai.com/docs/api-reference/introduction)

### Openrouter.ai

[https://openrouter.ai/docs/quickstart](https://openrouter.ai/docs/quickstart)

```bash
curl https://openrouter.ai/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENROUTER_API_KEY" \
  -d '{
  "model": "openai/gpt-4o",
  "messages": [
    {
      "role": "user",
      "content": "What is the meaning of life?"
    }
  ]
}'

```

## Utiliser une lib

## Avec llama.cpp

[https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)

# Avec node-llama

[https://node-llama-cpp.withcat.ai/](https://node-llama-cpp.withcat.ai/)

```bash
npx --no node-llama-cpp inspect gpu
```

# Int√©gration de l'IA dans Yeti avec Xcraft

## Xcraft - Le framework maison

[üìò documentation](../../../doc/autogen/xcraft-core-goblin/readme.md)

## Agents - Les agents maison

[üß† un agent](../../../lib/goblin-agents/lib/llm/yetiAgent.js)

[üìò documentation](../../../doc/autogen/goblin-agents/llm.md)

## Cryo - Le data-layer maison

[üìò documentation](../../../doc/autogen/xcraft-core-goblin/cryo.md)

### Embeddings

```sql
CREATE VIRTUAL TABLE IF NOT EXISTS embeddings768 USING vec0(
    documentId TEXT,
    chunkId TEXT,
    chunk TEXT,
    embedding FLOAT[768] distance_metric=cosine
)


SELECT documentId, chunkId, chunk, distance
      FROM embeddings768
      WHERE embedding match $vectors
      ORDER BY distance LIMIT $limit
```

## Indexeur de contenu

```js
class ChunkShape {
  chunk = string;
  embedding = array;
}

class MetaShape {
  index = option(string); //FullTextSearch
  status = enumeration('published', 'trashed', 'archived');
  vectors = option(record(string, ChunkShape)); //SQLITE-VEC
}

class IndexedContentShape {
  id = id('indexedContent');
  kind = string;
  title = string;
  description = string;
  contextId = string;
  sourceId = string;
  relatedIds = array(string);
  weight = number;
  meta = MetaShape;
}
```

[üß™ indexeur de contenu](../../../lib/goblin-yennefer/lib/indexer.js)

## Articles wordpress

[üìú ragCresus](../../../lib/goblin-yennefer/lib/prompts/ragCresus.md)

[üß™ processus RAG dans wordpress](../../../lib/goblin-epsilon/lib/wordpress/wordpress.js)

[üìò documentation de la partie Wordpress](../../../doc/autogen/goblin-epsilon/wordpress.md)

## Agents de bases

[üß† d√©finition des agents de base](../../../lib/goblin-yennefer/lib/builtInAgents.js)

[üìò documentation de la partie LLM](../../../doc/autogen/goblin-yennefer/llm.md)

## G√©n√©rer de la doc

[üìú codeAnalyser](../../../lib/goblin-yennefer/lib/prompts/codeAnalyser.md)

[üß™ g√©n√©rateur de doc](../../../lib/goblin-yennefer/lib/codeMiner.js)

## G√©n√©rer un dataset

[üìú qaMinerDatasetBuilder](../../../lib/goblin-yennefer/lib/prompts/qaMinerDatasetBuilder.md)

[üß™ g√©n√©rateur de Question&Answer ](../../../lib/goblin-yennefer/lib/qaMiner.js)

## G√©n√©ration de t√¢ches et analyse de cas

[üìú tasksProposalAgent](../../../lib/goblin-yennefer/lib/prompts/tasksProposalAgent.md)

[üìú agentOrchestrator](../../../lib/goblin-yennefer/lib/prompts/agentOrchestrator.md)

[üìú promptGen](../../../lib/goblin-yennefer/lib/prompts/promptGen.md)

[üß™ analyse de cas](../../../lib/goblin-yeti/lib/widgets/case-workitem/service.js)

[üìò documentation du case-workitem](../../../doc/autogen/goblin-yeti/case-workitem.md)

## Dans le journal

[üß™ commander des agents](../../../lib/goblin-yeti/lib/widgets/journal/service.js)

[üìò documentation du journal](../../../doc/autogen/goblin-yeti/journal.md)

# Bonus : extraction et analyse de PDF

```js
//TODO
```
